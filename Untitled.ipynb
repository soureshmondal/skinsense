{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3efae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pybind11>=2.12 in c:\\users\\soure\\anaconda3\\lib\\site-packages (2.13.6)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \"pybind11>=2.12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b97f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\soure\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numexpr in c:\\users\\soure\\anaconda3\\lib\\site-packages (2.8.1)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.2-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: bottleneck in c:\\users\\soure\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Collecting bottleneck\n",
      "  Downloading Bottleneck-1.4.2-cp39-cp39-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading numexpr-2.10.2-cp39-cp39-win_amd64.whl (144 kB)\n",
      "Downloading Bottleneck-1.4.2-cp39-cp39-win_amd64.whl (111 kB)\n",
      "Installing collected packages: numexpr, bottleneck\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.8.1\n",
      "    Uninstalling numexpr-2.8.1:\n",
      "      Successfully uninstalled numexpr-2.8.1\n",
      "  Attempting uninstall: bottleneck\n",
      "    Found existing installation: Bottleneck 1.3.4\n",
      "    Uninstalling Bottleneck-1.3.4:\n",
      "      Successfully uninstalled Bottleneck-1.3.4\n",
      "Successfully installed bottleneck-1.4.2 numexpr-2.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas numexpr bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202e9dd",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b3bbcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soure\\AppData\\Local\\Temp\\ipykernel_18408\\1308633150.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['age'].fillna(df['age'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training images...\n",
      "Processing validation images...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    SEED = 42\n",
    "    DATA_DIR = r\"D:\\programs\\hackathon\\dataverse_files\\HAM10000_images_combined_600x450\"\n",
    "    METADATA_PATH = r\"D:\\programs\\hackathon\\dataverse_files\\HAM10000_metadata\"\n",
    "    PROCESSED_DATA_DIR = \"processed_data\"\n",
    "    IMAGE_SIZE = (224, 224)\n",
    "    NUM_WORKERS = min(8, os.cpu_count())\n",
    "\n",
    "def preprocess_images(image_path, transform, save_path):\n",
    "    \"\"\"Process and save individual images\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "        torch.save(img_tensor, save_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \n",
    "    os.makedirs(Config.PROCESSED_DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(Config.METADATA_PATH)\n",
    "    df['age'].fillna(df['age'].median(), inplace=True)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['dx_encoded'] = label_encoder.fit_transform(df['dx'])\n",
    "    \n",
    "    \n",
    "    with open(os.path.join(Config.PROCESSED_DATA_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    \n",
    "    train_meta, val_meta = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2, \n",
    "        stratify=df['dx_encoded'], \n",
    "        random_state=Config.SEED\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_meta.to_csv(os.path.join(Config.PROCESSED_DATA_DIR, 'train_metadata.csv'), index=False)\n",
    "    val_meta.to_csv(os.path.join(Config.PROCESSED_DATA_DIR, 'val_metadata.csv'), index=False)\n",
    "    \n",
    "    \n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize(Config.IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "\n",
    "    train_dir = os.path.join(Config.PROCESSED_DATA_DIR, 'train_images')\n",
    "    val_dir = os.path.join(Config.PROCESSED_DATA_DIR, 'val_images')\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    print(\"Processing training images...\")\n",
    "    for img_id in train_meta['image_id']:\n",
    "        img_path = os.path.join(Config.DATA_DIR, f\"{img_id}.jpg\")\n",
    "        save_path = os.path.join(train_dir, f\"{img_id}.pt\")\n",
    "        preprocess_images(img_path, base_transform, save_path)\n",
    "    \n",
    "    \n",
    "    print(\"Processing validation images...\")\n",
    "    for img_id in val_meta['image_id']:\n",
    "        img_path = os.path.join(Config.DATA_DIR, f\"{img_id}.jpg\")\n",
    "        save_path = os.path.join(val_dir, f\"{img_id}.pt\")\n",
    "        preprocess_images(img_path, base_transform, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858e2cd",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174986a",
   "metadata": {},
   "source": [
    "## Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43c0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:Loading training dataset...\n",
      "INFO:__main__:Dataset initialized with 8012 images\n",
      "INFO:__main__:Loading validation dataset...\n",
      "INFO:__main__:Dataset initialized with 2003 images\n",
      "Epoch 1/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 1/30 [Train]: 100%|███████████████████████████████████| 501/501 [06:53<00:00,  1.21it/s, loss=0.7124, acc=74.53%]\n",
      "Epoch 1/30 [Val]: 100%|█████████████████████████████████████| 126/126 [01:15<00:00,  1.67it/s, loss=0.0324, acc=81.48%]\n",
      "INFO:__main__:Epoch 1: Train Loss=0.7124, Train Acc=74.53%, Val Loss=0.5146, Val Acc=81.48%\n",
      "Epoch 2/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 2/30 [Train]: 100%|███████████████████████████████████| 501/501 [06:36<00:00,  1.26it/s, loss=0.4856, acc=82.16%]\n",
      "Epoch 2/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:51<00:00,  2.43it/s, loss=0.0926, acc=68.35%]\n",
      "INFO:__main__:Epoch 2: Train Loss=0.4856, Train Acc=82.16%, Val Loss=1.4713, Val Acc=68.35%\n",
      "Epoch 3/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 3/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:13<00:00,  1.60it/s, loss=0.3312, acc=88.35%]\n",
      "Epoch 3/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:50<00:00,  2.51it/s, loss=0.0352, acc=81.43%]\n",
      "INFO:__main__:Epoch 3: Train Loss=0.3312, Train Acc=88.35%, Val Loss=0.5600, Val Acc=81.43%\n",
      "Epoch 4/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 4/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:10<00:00,  1.61it/s, loss=0.2086, acc=92.96%]\n",
      "Epoch 4/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:50<00:00,  2.51it/s, loss=0.0665, acc=75.69%]\n",
      "INFO:__main__:Epoch 4: Train Loss=0.2086, Train Acc=92.96%, Val Loss=1.0579, Val Acc=75.69%\n",
      "Epoch 5/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 5/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:10<00:00,  1.61it/s, loss=0.1535, acc=94.80%]\n",
      "Epoch 5/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:50<00:00,  2.52it/s, loss=0.1382, acc=64.65%]\n",
      "INFO:__main__:Epoch 5: Train Loss=0.1535, Train Acc=94.80%, Val Loss=2.1963, Val Acc=64.65%\n",
      "Epoch 6/30 [Train]:   0%|                                                                      | 0/501 [00:00<?, ?it/s]INFO:__main__:Batch shape: torch.Size([16, 3, 224, 224])\n",
      "Epoch 6/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:10<00:00,  1.61it/s, loss=0.1127, acc=96.49%]\n",
      "Epoch 6/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:50<00:00,  2.49it/s, loss=0.1379, acc=67.55%]\n",
      "INFO:__main__:Epoch 6: Train Loss=0.1127, Train Acc=96.49%, Val Loss=2.1926, Val Acc=67.55%\n",
      "INFO:__main__:Early stopping at epoch 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    BATCH_SIZE = 16  \n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_CLASSES = 7\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    PATIENCE = 5\n",
    "    NUM_WORKERS = 0  \n",
    "    PROCESSED_DATA_DIR = \"processed_data\"\n",
    "    IMG_SIZE = (3, 224, 224)  \n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    def __init__(self, metadata_path, images_dir):\n",
    "        self.metadata_path = Path(metadata_path)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        \n",
    "        if not self.metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n",
    "        if not self.images_dir.exists():\n",
    "            raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "            \n",
    "        \n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        required_columns = ['image_id', 'dx_encoded']\n",
    "        if not all(col in self.metadata.columns for col in required_columns):\n",
    "            raise ValueError(f\"Metadata must contain columns: {required_columns}\")\n",
    "        \n",
    "        \n",
    "        self.image_paths = []\n",
    "        for img_id in self.metadata['image_id']:\n",
    "            img_path = self.images_dir / f\"{img_id}.pt\"\n",
    "            if not img_path.exists():\n",
    "                raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "            self.image_paths.append(img_path)\n",
    "        \n",
    "        \n",
    "        self.labels = torch.tensor(self.metadata['dx_encoded'].values, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        first_image = torch.load(self.image_paths[0])\n",
    "        if first_image.shape != Config.IMG_SIZE:\n",
    "            raise ValueError(f\"Expected image shape {Config.IMG_SIZE}, got {first_image.shape}\")\n",
    "        \n",
    "        logger.info(f\"Dataset initialized with {len(self.metadata)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            \n",
    "            image = torch.load(self.image_paths[idx], weights_only=True)\n",
    "            \n",
    "            \n",
    "            if image.shape != Config.IMG_SIZE:\n",
    "                raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "            \n",
    "            \n",
    "            image = image.float()\n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "                \n",
    "            return image, self.labels[idx]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading item {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(Config.DEVICE)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{Config.NUM_EPOCHS} [Train]')\n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                logger.info(f\"Batch shape: {images.shape}\")\n",
    "            \n",
    "            images = images.to(Config.DEVICE)\n",
    "            labels = labels.to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "            \n",
    "            \n",
    "            del images, labels, outputs, loss\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{Config.NUM_EPOCHS} [Val]')\n",
    "            for images, labels in val_pbar:\n",
    "                images = images.to(Config.DEVICE)\n",
    "                labels = labels.to(Config.DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/val_total:.4f}',\n",
    "                    'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "                })\n",
    "                \n",
    "                del images, labels, outputs, loss\n",
    "\n",
    "        val_acc = 100.*val_correct/val_total\n",
    "        \n",
    "        \n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Train Loss={train_loss/len(train_loader):.4f}, \"\n",
    "            f\"Train Acc={100.*train_correct/train_total:.2f}%, \"\n",
    "            f\"Val Loss={val_loss/len(val_loader):.4f}, \"\n",
    "            f\"Val Acc={val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == Config.PATIENCE:\n",
    "                logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        \n",
    "        logger.info(f\"Using device: {Config.DEVICE}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        \n",
    "        logger.info(\"Loading training dataset...\")\n",
    "        train_dataset = ProcessedDataset(\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'train_metadata.csv'),\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'train_images')\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Loading validation dataset...\")\n",
    "        val_dataset = ProcessedDataset(\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'val_metadata.csv'),\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'val_images')\n",
    "        )\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=Config.NUM_WORKERS\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=Config.NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        \n",
    "        model = create_model(Config.NUM_CLASSES)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(\n",
    "            [p for p in model.parameters() if p.requires_grad],\n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "        \n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d6ad2",
   "metadata": {},
   "source": [
    "## Version 2 (working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8008829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n",
      "INFO:__main__:Loading training dataset...\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:dx_encoded\n",
      "5    5364\n",
      "4     890\n",
      "2     879\n",
      "1     411\n",
      "0     262\n",
      "6     114\n",
      "3      92\n",
      "Name: count, dtype: int64\n",
      "INFO:__main__:Dataset initialized with 8012 images\n",
      "INFO:__main__:Loading validation dataset...\n",
      "INFO:__main__:Class distribution:\n",
      "INFO:__main__:dx_encoded\n",
      "5    1341\n",
      "4     223\n",
      "2     220\n",
      "1     103\n",
      "0      65\n",
      "6      28\n",
      "3      23\n",
      "Name: count, dtype: int64\n",
      "INFO:__main__:Dataset initialized with 2003 images\n",
      "Epoch 1/30 [Train]: 100%|███████████████████████████████████| 501/501 [08:03<00:00,  1.04it/s, loss=3.6362, acc=44.31%]\n",
      "Epoch 1/30 [Val]: 100%|█████████████████████████████████████| 126/126 [01:21<00:00,  1.55it/s, loss=1.7792, acc=32.70%]\n",
      "INFO:__main__:Epoch 1: Train Loss=3.6362, Train Acc=44.31%, Val Loss=1.7792, Val Acc=32.70%\n",
      "Epoch 2/30 [Train]: 100%|███████████████████████████████████| 501/501 [06:11<00:00,  1.35it/s, loss=1.7624, acc=47.80%]\n",
      "Epoch 2/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:55<00:00,  2.26it/s, loss=1.4437, acc=48.83%]\n",
      "INFO:__main__:Epoch 2: Train Loss=1.7624, Train Acc=47.80%, Val Loss=1.4437, Val Acc=48.83%\n",
      "Epoch 3/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:53<00:00,  1.42it/s, loss=1.4864, acc=52.12%]\n",
      "Epoch 3/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:51<00:00,  2.44it/s, loss=2.2516, acc=31.30%]\n",
      "INFO:__main__:Epoch 3: Train Loss=1.4864, Train Acc=52.12%, Val Loss=2.2516, Val Acc=31.30%\n",
      "Epoch 4/30 [Train]: 100%|███████████████████████████████████| 501/501 [05:44<00:00,  1.45it/s, loss=1.4045, acc=54.66%]\n",
      "Epoch 4/30 [Val]: 100%|█████████████████████████████████████| 126/126 [00:51<00:00,  2.46it/s, loss=1.7066, acc=47.08%]\n",
      "INFO:__main__:Epoch 4: Train Loss=1.4045, Train Acc=54.66%, Val Loss=1.7066, Val Acc=47.08%\n",
      "Epoch 5/30 [Train]: 100%|███████████████████████████████████| 501/501 [07:08<00:00,  1.17it/s, loss=1.4435, acc=51.90%]\n",
      "Epoch 5/30 [Val]: 100%|█████████████████████████████████████| 126/126 [01:23<00:00,  1.50it/s, loss=1.8856, acc=32.90%]\n",
      "INFO:__main__:Epoch 5: Train Loss=1.4435, Train Acc=51.90%, Val Loss=1.8856, Val Acc=32.90%\n",
      "Epoch 6/30 [Train]: 100%|███████████████████████████████████| 501/501 [09:10<00:00,  1.10s/it, loss=1.2508, acc=55.10%]\n",
      "Epoch 6/30 [Val]: 100%|█████████████████████████████████████| 126/126 [01:24<00:00,  1.50it/s, loss=2.1972, acc=13.98%]\n",
      "INFO:__main__:Epoch 6: Train Loss=1.2508, Train Acc=55.10%, Val Loss=2.1972, Val Acc=13.98%\n",
      "Epoch 7/30 [Train]: 100%|███████████████████████████████████| 501/501 [09:10<00:00,  1.10s/it, loss=1.2540, acc=54.89%]\n",
      "Epoch 7/30 [Val]: 100%|█████████████████████████████████████| 126/126 [01:23<00:00,  1.50it/s, loss=1.6627, acc=34.60%]\n",
      "INFO:__main__:Epoch 7: Train Loss=1.2540, Train Acc=54.89%, Val Loss=1.6627, Val Acc=34.60%\n",
      "INFO:__main__:Early stopping at epoch 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    BATCH_SIZE = 16  \n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_CLASSES = 7\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    PATIENCE = 5\n",
    "    NUM_WORKERS = 0  \n",
    "    PROCESSED_DATA_DIR = \"processed_data\"\n",
    "    IMG_SIZE = (3, 224, 224)  \n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    def __init__(self, metadata_path, images_dir, transform=None):\n",
    "        self.metadata_path = Path(metadata_path)\n",
    "        self.images_dir = Path(images_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not self.metadata_path.exists():\n",
    "            raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n",
    "        if not self.images_dir.exists():\n",
    "            raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "            \n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        required_columns = ['image_id', 'dx_encoded']\n",
    "        if not all(col in self.metadata.columns for col in required_columns):\n",
    "            raise ValueError(f\"Metadata must contain columns: {required_columns}\")\n",
    "        \n",
    "        logger.info(\"Class distribution:\")\n",
    "        logger.info(self.metadata['dx_encoded'].value_counts())\n",
    "        \n",
    "        self.image_paths = []\n",
    "        for img_id in self.metadata['image_id']:\n",
    "            img_path = self.images_dir / f\"{img_id}.pt\"\n",
    "            if not img_path.exists():\n",
    "                raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "            self.image_paths.append(img_path)\n",
    "        \n",
    "        self.labels = torch.tensor(self.metadata['dx_encoded'].values, dtype=torch.long)\n",
    "        \n",
    "        first_image = torch.load(self.image_paths[0])\n",
    "        if first_image.shape != Config.IMG_SIZE:\n",
    "            raise ValueError(f\"Expected image shape {Config.IMG_SIZE}, got {first_image.shape}\")\n",
    "        \n",
    "        logger.info(f\"Dataset initialized with {len(self.metadata)} images\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = torch.load(self.image_paths[idx], weights_only=True)\n",
    "            image = image.float()\n",
    "            \n",
    "            if image.max() > 1.0:\n",
    "                image = image / 255.0\n",
    "                \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, self.labels[idx]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading item {idx}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    \n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    model.fc.apply(init_weights)\n",
    "    return model.to(Config.DEVICE)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer):\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{Config.NUM_EPOCHS} [Train]')\n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            images = images.to(Config.DEVICE)\n",
    "            labels = labels.to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{train_loss/(batch_idx+1):.4f}',\n",
    "                'acc': f'{100.*train_correct/train_total:.2f}%'\n",
    "            })\n",
    "            \n",
    "            del images, labels, outputs, loss\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{Config.NUM_EPOCHS} [Val]')\n",
    "            for images, labels in val_pbar:\n",
    "                images = images.to(Config.DEVICE)\n",
    "                labels = labels.to(Config.DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{val_loss/len(val_loader):.4f}',\n",
    "                    'acc': f'{100.*val_correct/val_total:.2f}%'\n",
    "                })\n",
    "\n",
    "        val_acc = 100.*val_correct/val_total\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Train Loss={train_loss/len(train_loader):.4f}, \"\n",
    "            f\"Train Acc={100.*train_correct/train_total:.2f}%, \"\n",
    "            f\"Val Loss={val_loss/len(val_loader):.4f}, \"\n",
    "            f\"Val Acc={val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == Config.PATIENCE:\n",
    "                logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logger.info(f\"Using device: {Config.DEVICE}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        \n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=20),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        logger.info(\"Loading training dataset...\")\n",
    "        train_dataset = ProcessedDataset(\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'train_metadata.csv'),\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'train_images'),\n",
    "            transform=train_transform\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Loading validation dataset...\")\n",
    "        val_dataset = ProcessedDataset(\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'val_metadata.csv'),\n",
    "            os.path.join(Config.PROCESSED_DATA_DIR, 'val_images'),\n",
    "            transform=val_transform\n",
    "        )\n",
    "        \n",
    "        \n",
    "        train_labels = train_dataset.metadata['dx_encoded'].values\n",
    "        classes = np.unique(train_labels)\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(Config.DEVICE)\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=Config.NUM_WORKERS\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=Config.NUM_WORKERS\n",
    "        )\n",
    "\n",
    "        \n",
    "        model = create_model(Config.NUM_CLASSES)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.AdamW(\n",
    "            [p for p in model.parameters() if p.requires_grad],\n",
    "            lr=Config.LEARNING_RATE,\n",
    "            weight_decay=1e-4\n",
    "        )\n",
    "\n",
    "    \n",
    "        train_model(model, train_loader, val_loader, criterion, optimizer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c89339",
   "metadata": {},
   "source": [
    "## Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4a3563",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 126\u001b[0m\n\u001b[0;32m    123\u001b[0m     train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     96\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(\u001b[38;5;241m224\u001b[39m, scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[0;32m     97\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m    103\u001b[0m ])\n\u001b[0;32m    104\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    105\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m    106\u001b[0m ])\n\u001b[1;32m--> 108\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mProcessedDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_metadata.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ProcessedDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_metadata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_images\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mval_transform)\n\u001b[0;32m    111\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mProcessedDataset.__init__\u001b[1;34m(self, metadata_path, images_dir, transform)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, metadata_path, images_dir, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir \u001b[38;5;241m=\u001b[39m images_dir\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_metadata.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "class Config:\n",
    "    BATCH_SIZE = 16  \n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 3e-4\n",
    "    NUM_CLASSES = 7\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    PATIENCE = 5\n",
    "    IMG_SIZE = (3, 224, 224)  \n",
    "    GRAD_ACCUM_STEPS = 2  # Gradient accumulation steps\n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    def __init__(self, metadata_path, images_dir, transform=None):\n",
    "        self.metadata = pd.read_csv(metadata_path)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(images_dir, f\"{img_id}.pt\") for img_id in self.metadata['image_id']]\n",
    "        self.labels = torch.tensor(self.metadata['dx_encoded'].values, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.load(self.image_paths[idx]).float()\n",
    "        if image.max() > 1.0:\n",
    "            image /= 255.0\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "def create_model(num_classes):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.layer3.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    return model.to(Config.DEVICE)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler):\n",
    "    best_val_acc = 0.0\n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) / Config.GRAD_ACCUM_STEPS\n",
    "            loss.backward()\n",
    "            if (batch_idx + 1) % Config.GRAD_ACCUM_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        scheduler.step(val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'model_best_{epoch+1}.pth')\n",
    "\n",
    "def main():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        transforms.GaussianBlur(3),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_dataset = ProcessedDataset('train_metadata.csv', 'train_images', transform=train_transform)\n",
    "    val_dataset = ProcessedDataset('val_metadata.csv', 'val_images', transform=val_transform)\n",
    "    \n",
    "    train_labels = train_dataset.metadata['dx_encoded'].values\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(Config.DEVICE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = create_model(Config.NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=Config.LEARNING_RATE, weight_decay=5e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a943254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soure\\AppData\\Local\\Temp\\ipykernel_14252\\336314359.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample = torch.load(r\"D:\\programs\\hackathon\\processed_data\\train_images\\ISIC_0024306.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sample = torch.load(r\"D:\\programs\\hackathon\\processed_data\\train_images\\ISIC_0024306.pt\")\n",
    "print(sample.shape, sample.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e3dc6",
   "metadata": {},
   "source": [
    "## app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d08f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to your image: D:\\programs\\hackathon\\dataverse_files\\HAM10000_images_combined_600x450\\ISIC_0024702.jpg\n",
      "\n",
      "Predicted class: Melanocytic nevi\n",
      "\n",
      "Confidence scores:\n",
      "Actinic keratoses / intraepithelial carcinoma: 0.0%\n",
      "Basal cell carcinoma: 0.0%\n",
      "Benign keratosis-like lesions: 0.0%\n",
      "Dermatofibroma : 0.0%\n",
      "Melanoma: 0.0%\n",
      "Melanocytic nevi: 100.0%\n",
      "Vascular lesions: 0.0%\n",
      "\n",
      "Visualization saved as: ISIC_0024702_prediction.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "class LesionClassifier:\n",
    "    def __init__(self, model_path, device=None):\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.class_names = ['Actinic keratoses / intraepithelial carcinoma', 'Basal cell carcinoma', 'Benign keratosis-like lesions', 'Dermatofibroma ', 'Melanoma', 'Melanocytic nevi', 'Vascular lesions']\n",
    "        self.model = self._load_model(model_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _load_model(self, model_path):\n",
    "\n",
    "        model = models.resnet18(weights=None)\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, len(self.class_names))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "            predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "        return {\n",
    "            'class_name': self.class_names[predicted_class],\n",
    "            'probabilities': probabilities.cpu().numpy(),\n",
    "            'original_image': image\n",
    "        }\n",
    "\n",
    "    def visualize_prediction(self, prediction, save_path=None):\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "        \n",
    "        \n",
    "        ax1.imshow(prediction['original_image'])\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        \n",
    "        probabilities = prediction['probabilities'] * 100\n",
    "        y_pos = np.arange(len(self.class_names))\n",
    "        \n",
    "        bars = ax2.barh(y_pos, probabilities)\n",
    "        ax2.set_yticks(y_pos)\n",
    "        ax2.set_yticklabels(self.class_names)\n",
    "        ax2.set_xlabel('Confidence (%)')\n",
    "        ax2.set_title('Prediction Confidence')\n",
    "        \n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax2.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{width:.1f}%',\n",
    "                    va='center')\n",
    "        \n",
    "        \n",
    "        predicted_idx = self.class_names.index(prediction['class_name'])\n",
    "        bars[predicted_idx].set_color('red')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    model_path = 'best_model.pth'  \n",
    "    classifier = LesionClassifier(model_path)\n",
    "    \n",
    "    \n",
    "    image_path = input(\"Enter the path to your image: \")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        prediction = classifier.predict(image_path)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nPredicted class: {prediction['class_name']}\")\n",
    "        print(\"\\nConfidence scores:\")\n",
    "        for class_name, prob in zip(classifier.class_names, prediction['probabilities']):\n",
    "            print(f\"{class_name}: {prob*100:.1f}%\")\n",
    "        \n",
    "        # Visualize results\n",
    "        save_path = Path(image_path).stem + \"_prediction.png\"\n",
    "        classifier.visualize_prediction(prediction, save_path)\n",
    "        print(f\"\\nVisualization saved as: {save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd0439d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d546f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting openai\n",
      "  Downloading openai-1.60.2-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from openai) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp39-cp39-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: sniffio in c:\\users\\soure\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\soure\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.4)\n",
      "Downloading openai-1.60.2-py3-none-any.whl (456 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.8.2-cp39-cp39-win_amd64.whl (207 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 560.1 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 560.1 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/2.0 MB 657.8 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 680.3 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 680.3 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 680.3 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 599.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 599.2 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 599.2 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.6/2.0 MB 566.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 610.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 623.2 kB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.60.2 pydantic-2.10.6 pydantic-core-2.27.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4685dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\soure\\anaconda3\\lib\\site-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\soure\\anaconda3\\lib\\site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\soure\\anaconda3\\lib\\site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soure\\anaconda3\\lib\\site-packages (from sacremoses) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\soure\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.4)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 897.5/897.5 kB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe272e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9e6f409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 'microsoft/BioGPT'...\n",
      "Model 'microsoft/BioGPT' loaded successfully on cpu.\n",
      "Model Max Length: 1000000000000000019884624838656\n",
      "Starting text processing with open-source model...\n",
      "Input Tokens: torch.Size([1, 58])\n",
      "Generated Output: Analyze the following topic in detail: - Define cancer and its key characteristics. - Explain the types of cancer and common causes. - Provide examples of prevention methods and treatments. Describe the key features of lung cancer, including causes, symptoms, and treatment options.\n",
      "Analysis completed and saved successfully!\n",
      "Analysis: Analyze the following topic in detail: - Define cancer and its key characteristics. - Explain the types of cancer and common causes. - Provide examples of prevention methods and treatments. Describe the key features of lung cancer, including causes, symptoms, and treatment options.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any, Optional\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class OpenSourceLLMProcessor:\n",
    "    def __init__(self, model_name: str = \"microsoft/BioGPT\"):\n",
    "        \"\"\"\n",
    "        Initialize the LLM processor using Hugging Face transformers.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        print(f\"Loading model '{self.model_name}'...\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Move model to appropriate device\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        print(f\"Model '{self.model_name}' loaded successfully on {self.device}.\")\n",
    "        print(f\"Model Max Length: {self.tokenizer.model_max_length}\")\n",
    "\n",
    "    def process_text(self, input_text: str, \n",
    "                     system_prompt: Optional[str] = None,\n",
    "                     max_tokens: int = 300,  # Adjust as needed\n",
    "                     temperature: float = 0.7) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process text using the Hugging Face transformers library.\n",
    "        \"\"\"\n",
    "        if not system_prompt:\n",
    "            system_prompt = \"\"\"Analyze the following topic in detail:\n",
    "- Define cancer and its key characteristics.\n",
    "- Explain the types of cancer and common causes.\n",
    "- Provide examples of prevention methods and treatments.\"\"\"\n",
    "\n",
    "        # Combine system prompt and user input\n",
    "        full_prompt = f\"{system_prompt}\\n\\n{input_text}\"\n",
    "\n",
    "        try:\n",
    "            # Tokenize input text\n",
    "            input_ids = self.tokenizer.encode(full_prompt, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            print(f\"Input Tokens: {input_ids.shape}\")\n",
    "            \n",
    "            # Generate response\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                max_length=max_tokens,  # Fix max_length\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Decode and return the result\n",
    "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            print(f\"Generated Output: {generated_text}\")\n",
    "\n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"analysis\": generated_text,\n",
    "                \"model_used\": self.model_name,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"input_length\": len(input_text),\n",
    "                \"metadata\": {\n",
    "                    \"temperature\": temperature,\n",
    "                    \"max_tokens\": max_tokens\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def save_results(self, results: Dict[str, Any], output_path: str):\n",
    "        \"\"\"\n",
    "        Save the analysis results to a JSON file.\n",
    "        \"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage with Hugging Face's transformers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processor = OpenSourceLLMProcessor(model_name=\"microsoft/BioGPT\")\n",
    "\n",
    "        # Using a shorter sample text\n",
    "        sample_text = \"Describe the key features of lung cancer, including causes, symptoms, and treatment options.\"\n",
    "\n",
    "        print(\"Starting text processing with open-source model...\")\n",
    "        results = processor.process_text(\n",
    "            input_text=sample_text,\n",
    "            max_tokens=300,  # Adjust as needed\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        if results[\"success\"]:\n",
    "            processor.save_results(results, \"analysis_results.json\")\n",
    "            print(\"Analysis completed and saved successfully!\")\n",
    "            print(f\"Analysis: {results['analysis']}\")\n",
    "        else:\n",
    "            print(f\"Error occurred: {results['error']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bd8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0765c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
